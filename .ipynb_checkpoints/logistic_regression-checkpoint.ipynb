{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbff3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    #load the train files\n",
    "    df = None\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 5,7 ):\n",
    "        \n",
    "        tmp = pd.read_csv( 'data/train%d.txt' % i, header=None, sep=\" \" )\n",
    "                \n",
    "        #build labels - one hot vector\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(5,7) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    train_data = df.to_numpy()\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    #load test files\n",
    "    df = None\n",
    "    \n",
    "    y_test = []\n",
    "\n",
    "    for i in range( 5,7 ):\n",
    "        \n",
    "        tmp = pd.read_csv( 'data/test%d.txt' % i, header=None, sep=\" \" )\n",
    "        \n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(5,7) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    test_data = df.to_numpy()\n",
    "    y_test = np.array( y_test )\n",
    "    \n",
    "    return train_data, test_data, y_train, y_test\n",
    "\n",
    "\n",
    "#Load the dataset.\n",
    "X_train, X_test, y_train, y_test = load_data()\n",
    "\n",
    "#split our training dataset to train and validation set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, train_size=0.8, random_state=42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7bef9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4071ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x0000021DEE515AC0>\n"
     ]
    }
   ],
   "source": [
    "#print(x.values for x in X_train if x.values != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc3fe8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#Reshaping our images from 28*28 to 784-element vectors.\n",
    "\n",
    "def create_feature_vectors(num_of_features, data):\n",
    "    \n",
    "    vector = data.reshape([-1, num_of_features])\n",
    "    return vector\n",
    "\n",
    "\n",
    "#Creating training, test and validation vectors.\n",
    "create_feature_vectors(784, X_train)\n",
    "create_feature_vectors(784, X_test)\n",
    "create_feature_vectors(784, X_val)    \n",
    "\n",
    "#normalize data to get values between (0,1)\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255\n",
    "X_val = X_val.astype(float) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5fa6710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9071, 784)\n",
      "(2268, 784)\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ab0173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meros B - Logistic Regression\n",
    "\n",
    "#We add a column of 1s to each data vector.\n",
    "#X_train = np.hstack( (np.ones((X_train.shape[0], 1) ), X_train))\n",
    "#X_test = np.hstack( (np.ones((X_test.shape[0], 1) ), X_test))\n",
    "#X_val = np.hstack( (np.ones((X_val.shape[0], 1) ), X_val))\n",
    "\n",
    "m, n = X_train.shape\n",
    "#We need this small value for the logs to avoid divided by zero warning.\n",
    "epsilon = 1e-5\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "#We won't use L2 regularization on this version\n",
    "def calculateCost(X, y, theta):\n",
    "    \n",
    "    h = sigmoid(X.dot(theta))\n",
    "    cur_J = (y.T.dot(np.log(h + epsilon)) + (1-y).T.dot(np.log(1 - h + epsilon)))\n",
    "    \n",
    "    #calculate gradient\n",
    "    gradient = X.T.dot(h-y)\n",
    "    \n",
    "    return cur_J, gradient\n",
    "\n",
    "def LogisticRegression(X, y, X_val, y_val, epoch = 2000, alpha = 0.01):\n",
    "    \n",
    "    theta = np.zeros([X.shape[1], 2])\n",
    "    print(theta.shape)\n",
    "    m, n = X.shape\n",
    "    \n",
    "    J_train = []\n",
    "    J_test = []\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        train_error, train_gradient = calculateCost(X, y, theta)\n",
    "        test_error, _ = calculateCost(X_val, y_val, theta)\n",
    "        \n",
    "        #update theta parameters, using gradient ascent.\n",
    "        theta += alpha * train_gradient\n",
    "        \n",
    "        J_train.append(train_error[0])\n",
    "        J_test.append(test_error[0])\n",
    "        \n",
    "    return J_train, J_test, theta\n",
    "\n",
    "\n",
    "def predict(theta, X):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #We will use 0.5 as the threshold to decide\n",
    "    #on which class it belongs\n",
    "    pred = np.zeros((m,1));\n",
    "    pred = sigmoid(np.dot(X, theta))\n",
    "    prob = pred\n",
    "    pred = pred > 0.5 - 1e-6\n",
    "    \n",
    "    return pred, prob\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9334377",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikos\\AppData\\Local\\Temp\\ipykernel_4984\\1286219214.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  cur_J = (y.T.dot(np.log(h)) + (1-y).T.dot(np.log(1 - h)))\n",
      "C:\\Users\\Nikos\\AppData\\Local\\Temp\\ipykernel_4984\\1286219214.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of training set 0.4753610406790872\n",
      "Accuracy of validation set 0.48897707231040566\n"
     ]
    }
   ],
   "source": [
    "#We train our model\n",
    "J_train, J_test, theta = LogisticRegression(X_train, y_train, X_val, y_val)\n",
    "\n",
    "#Predicting values\n",
    "pred_train, prob_train = predict(theta, X_train)\n",
    "pred_val, prob_val = predict(theta, X_val)\n",
    "\n",
    "print( 'Accuracy of training set', np.mean( pred_train.astype('int') == y_train ) )\n",
    "print( 'Accuracy of validation set', np.mean( pred_val.astype('int') == y_val ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6883bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will implement Logistic Regression\n",
    "#but this time with L2 Regularization.\n",
    "\n",
    "#the sigmoid function stays the same.\n",
    "\n",
    "def calculateCost(X, y, theta, _lambda):\n",
    "    \n",
    "    h = sigmoid(X.dot(theta))\n",
    "    \n",
    "    #calculate cost function with regularization\n",
    "    reg = (_lambda / 2.0) * np.sum(theta**2)\n",
    "    cur_J = (y.T.dot(np.log(h + epsilon)) + (1-y).T.dot(np.log(1 - h + epsilon))) - reg\n",
    "    \n",
    "    #calculate gradient\n",
    "    reg = _lambda * theta\n",
    "    gradient = (X.T.dot(h-y) ) - reg\n",
    "    \n",
    "    return cur_J, gradient\n",
    "\n",
    "def LogisticRegressionReg(X, y, X_val, y_val, _lambda = 0.0, epoch = 2000, alpha = 0.01):\n",
    "    \n",
    "    theta = np.zeros([X.shape[1], 2])\n",
    "    print(theta.shape)\n",
    "    #theta = np.zeros(X.shape[1]).reshape((-1,1))\n",
    "    m, n = X.shape\n",
    "    \n",
    "    J_train = []\n",
    "    J_test = []\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        train_error, train_gradient = calculateCost(X, y, theta, _lambda)\n",
    "        test_error, _ = calculateCost(X_val, y_val, theta, _lambda)\n",
    "        \n",
    "        #update theta parameters, using gradient ascent.\n",
    "        theta += alpha * train_gradient\n",
    "        #print(\"theta is: \", theta)\n",
    "        \n",
    "        J_train.append(train_error[0])\n",
    "        #print(\"J_train[i] is \", J_train[i])\n",
    "        J_test.append(test_error[0])\n",
    "        #print(\"J_test[i] is \", J_test[i])\n",
    "\n",
    "        \n",
    "    return J_train, J_test, theta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cef10631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikos\\AppData\\Local\\Temp\\ipykernel_18848\\73302964.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of training set 0.4753610406790872\n",
      "Accuracy of validation set 0.48897707231040566\n"
     ]
    }
   ],
   "source": [
    "J_train, J_test, theta = LogisticRegressionReg(X_train, y_train, X_val, y_val)\n",
    "\n",
    "#Predicting values\n",
    "pred_train, prob_train = predict(theta, X_train)\n",
    "pred_val, prob_val = predict(theta, X_val)\n",
    "\n",
    "print( 'Accuracy of training set', np.mean( pred_train.astype('int') == y_train ) )\n",
    "print( 'Accuracy of validation set', np.mean( pred_val.astype('int') == y_val ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1704a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
